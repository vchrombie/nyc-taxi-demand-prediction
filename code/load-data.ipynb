{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc4d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadef077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce52c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 23:31:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-vt2182:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbcd828ddb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a184e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"vt2182\"\n",
    "password = \"vt2182\"\n",
    "host = \"mongo-csgy-6513-spring.db\"\n",
    "auth_database = \"vt2182\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8da5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_conn_str = f\"mongodb://{username}:{password}@{host}/{auth_database}\"\n",
    "client = MongoClient(mongo_conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a62e000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['mongo-csgy-6513-spring.db:27017'], document_class=dict, tz_aware=False, connect=True), 'vt2182')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = client[auth_database]\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be19448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_tripdata_raw']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d04feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b806067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d18540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12532c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where your parquet files are stored\n",
    "path = \"../data/\"\n",
    "\n",
    "# Use glob to match the pattern ‘*.parquet’\n",
    "files = glob.glob(os.path.join(path, \"*.parquet\"))\n",
    "\n",
    "# Iterate over the list of files\n",
    "for file in files:\n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    # Check if the data frame has more than 10,000 rows\n",
    "    if len(df) > 10000:\n",
    "        # Randomly select 10,000 rows\n",
    "        df_sample = df.sample(n=10000)\n",
    "    else:\n",
    "        print(f\"The dataset in {file} has less than 10,000 rows.\")\n",
    "        continue\n",
    "\n",
    "    # Overwrite the parquet file with the sampled data\n",
    "    df_sample.to_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4bebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = spark.read.parquet('../data/yellow_tripdata_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ecc75bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da12505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2022, 12, 3, 19, 14, 47), tpep_dropoff_datetime=datetime.datetime(2022, 12, 3, 19, 31, 51), passenger_count=1.0, trip_distance=3.37, RatecodeID=1.0, store_and_fwd_flag='N', PULocationID=143, DOLocationID=152, payment_type=1, fare_amount=13.5, extra=0.5, mta_tax=0.5, tip_amount=3.46, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=20.76, congestion_surcharge=2.5, airport_fee=0.0, __index_level_0__=375653)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ffd7805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|__index_level_0__|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+\n",
      "|       2| 2022-12-03 19:14:47|  2022-12-03 19:31:51|            1.0|         3.37|       1.0|                 N|         143|         152|           1|       13.5|  0.5|    0.5|      3.46|         0.0|                  0.3|       20.76|                 2.5|        0.0|           375653|\n",
      "|       2| 2022-12-01 11:55:12|  2022-12-01 12:18:01|            6.0|          1.0|       1.0|                 N|         100|         230|           2|       13.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        17.8|                 2.5|        0.0|            71522|\n",
      "|       1| 2022-12-12 15:54:24|  2022-12-12 15:59:52|            1.0|          1.9|       1.0|                 N|         236|          74|           1|        7.5|  3.0|    0.5|      2.25|         0.0|                  0.3|       13.55|                 2.5|        0.0|          1402168|\n",
      "|       1| 2022-12-20 05:34:23|  2022-12-20 06:14:12|            1.0|          0.1|      99.0|                 N|         242|         242|           1|       21.5|  0.0|    0.5|       0.0|         0.0|                  1.0|        23.0|                 0.0|        0.0|          2262245|\n",
      "|       2| 2022-12-23 22:03:11|  2022-12-23 22:28:44|           null|         6.53|      null|              null|         164|          61|           0|      31.83|  0.0|    0.5|       3.0|         0.0|                  1.0|       38.83|                null|       null|          3383941|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556808de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 120000 rows.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows\n",
    "print(\"The data contains {} rows.\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d1571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "028927c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the collection\n",
    "collection = db['yellow_tripdata_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc7d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bigdata/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/opt/conda/envs/bigdata/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8639815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to a dictionary\n",
    "data_dict = pandas_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a212e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fbca49b1060>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the dictionary into MongoDB\n",
    "collection.insert_many(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff24c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
